\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm}
\usepackage{tikz}
\usepackage{float}
\usepackage{subcaption}

\title{Neural Network Solvers for Poisson Equations}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}
We consider the Poisson equation 
$$-\nabla^2 u = f \quad \text{in } \Omega$$
with Dirichlet boundary conditions 
$$u = g \quad \text{on } \partial \Omega$$
where $\Omega \subset \mathbb{R}^d$ is a domain, $\partial \Omega$ is its boundary, $f$ is a given source term, and $g$ is the boundary value.

\section{Architecture for incorporating boundary conditions}
To strictly impose the boundary condition $u = g$ on $\partial \Omega$, the candidate solution $u_{NN}(x; \theta)$ is constructed as:
$$u_{NN}(x; \theta) = g_0(x) + d(x) \tilde{u}(x; \theta)$$
where $g_0(x)$ is a function that satisfies $g_0|_{\partial\Omega} = g$, and $d(x)$ is a function that is zero on the boundary $\partial\Omega$ and positive in the interior $\Omega$. This construction ensures that $u_{NN}(x; \theta) = g(x)$ for $x \in \partial \Omega$.

\section{Deep Ritz Method (DRM)}

For the Poisson equation, the associated Ritz functional is:
$$J(u) = \int_{\Omega} \left( \frac{1}{2} |\nabla u|^2 - fu \right) \, dx$$
The DRM loss function is the discretized version of the Ritz functional, evaluated at collocation points $\{x_i\}_{i=1}^N$ sampled from $\Omega$:
$$L_{DRM}(\theta) = \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{2} |\nabla u_{NN}(x_i; \theta)|^2 - f(x_i) u_{NN}(x_i; \theta) \right)$$
This empirical average serves as an approximation of the continuous functional $J(u_{NN}(\theta))$. 

Let $u^\star$ be the analytical solution to the Poisson equation. Define the error as $v = u_{NN} - u^\star$. Since both $u_{NN}$ and $u^\star$ satisfy the boundary condition $u=g$ on $\partial\Omega$, we have $v=0$ on $\partial\Omega$.
Since $-\nabla^2 u^\star = f$, we have
\begin{align*}
\int_{\Omega} f v \, dx &= \int_{\Omega} (-\nabla^2 u^\star) v \, dx \\
&= \int_{\Omega} \nabla u^\star \cdot \nabla v \, dx - \int_{\partial\Omega} (\nabla u^\star \cdot \mathbf{n}) v \, dS \\
&= \int_{\Omega} \nabla u^\star \cdot \nabla v \, dx
\end{align*}
Now, substitute $u_{NN} = u^\star + v$ into the Ritz functional:
\begin{align*}
J(u_{NN}) &= \int_{\Omega} \left( \frac{1}{2} |\nabla (u^\star + v)|^2 - f (u^\star + v) \right) \, dx \\
&= \int_{\Omega} \left( \frac{1}{2} (|\nabla u^\star|^2 + 2 \nabla u^\star \cdot \nabla v + |\nabla v|^2) - f u^\star - f v \right) \, dx \\
&= \int_{\Omega} \left( \frac{1}{2} |\nabla u^\star|^2 - f u^\star \right) \, dx + \int_{\Omega} \left( \nabla u^\star \cdot \nabla v - f v \right) \, dx + \frac{1}{2} \int_{\Omega} |\nabla v|^2 \, dx \\
&= J(u^\star) + 0 + \frac{1}{2} \int_{\Omega} |\nabla (u_{NN} - u^\star)|^2 \, dx \\
&= J(u^\star) + \frac{1}{2} \vert u_{NN} - u^\star\vert_{H^1(\Omega)}^2
\end{align*}
This result shows that the difference between the Ritz functional evaluated at the neural network solution and the true solution is directly proportional to the squared $H^1$-seminorm of the approximation error. Therefore, minimizing $J(u_{NN})$ is equivalent to minimizing the $H^1$-seminorm of the approximation error.

The gradient of the DRM loss with respect to the neural network parameters $\theta$ is given by
\begin{align*}
    \nabla_{\theta} L_{DRM}(\theta)
    = \frac{1}{N} \sum_{i=1}^N \left(\nabla u_{NN}(x_i; \theta) \cdot \nabla_{\theta} \nabla u_{NN}(x_i; \theta) - f(x_i) \nabla_{\theta} u_{NN}(x_i; \theta)\right)
\end{align*}
This means the gradient computation involves second-order derivatives of $u_{NN}$. 
While the $H^1$-seminorm is a ``weaker" norm theoretically, the practical implications for optimizing neural networks using the DRM loss are significant:
\begin{enumerate}
\item  \textbf{Indirect Feedback:} The gradient indicates how changing $\theta$ affects the local energy density, instead of local PDE residual. While a global minimization of energy is equivalent to solving the PDE, the feedback mechanism is less direct and intuitive compared to PINNs. The network might satisfy the overall energy minimization but still have localized regions where the strong form of the PDE is poorly satisfied.
\item  \textbf{Noisy Global Estimate:} With a finite and often limited number of samples $N$ (due to the curse of dimensionality in higher dimensions), the Monte Carlo approximation of the integral can be noisy. The noise in the loss value directly translates to noisy gradients. This makes the optimization landscape appear rougher to standard gradient-based optimizers, hindering stable and efficient convergence. The optimizer might spend more iterations trying to find the ``true'' descent direction in account of the stochastic fluctuations.
\end{enumerate}


\section{Physics-Informed Neural Networks (PINN)}

For the Poisson equation, the associated $L^2$ resisual is:
$$\mathcal{E}(u) = \int_{\Omega} \vert \nabla^2 u + f \vert^2 \, dx$$
The PINN loss function is the discrete version of the $L^2$ residual,  evaluated at collocation points $\{x_i\}_{i=1}^N$ sampled from $\Omega$:
$$L_{PINN}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left(\nabla^2 u_{NN}(x_i; \theta) + f(x_i) \right)^2$$
This empirical average serves as an approximation of the continuous $L^2$ residual $\mathcal{E}(u_{NN}(\theta))$. 

Let $u^\star$ be the analytical solution to the Poisson equation. 
Define the error as $v = u_{NN} - u^\star$. Since both $u_{NN}$ and $u^\star$ satisfy the boundary condition $u=g$ on $\partial\Omega$, we have $v=0$ on $\partial\Omega$.
Assuming $\Omega$ is sufficiently regular, 
by Miranda-Talenti inequality, 
we have $$ C^{-1} \vert v \vert_{H^2(\Omega)} \leq ||\Delta v||_{L^2(\Omega)} \leq C \vert v \vert_{H^2(\Omega)} $$
where $C > 0$ is a constant for norm equivalence.
Since $-\nabla^2 u^\star = f$, we have 
\begin{align*}
\mathcal{E}(u_{NN}) &= \int_{\Omega} \vert \nabla^2 u_{NN} + f \vert^2 \, dx \\
& = \int_{\Omega} \vert \nabla^2 (u_{NN} - u^\star) \vert^2 \, dx,
\end{align*}
which implies 
$$ C^{-2} \vert u_{NN} - u^\star \vert_{H^2(\Omega)}^2 \leq \mathcal{E}(u_{NN}) \leq C^2 \vert u_{NN} - u^\star \vert_{H^2(\Omega)}^2 $$
Therefore, minimizing the PINN loss directly minimizes the $H^2$-seminorm of the approximation error, assuming the boundary conditions are strictly imposed. This is a stronger norm than the $H^1$-seminorm for DRM, suggesting that PINNs may lead to smoother solutions, although this also implies a higher requirement for the regularity of the true solution.

The gradient of the PINN loss with respect to the neural network parameters $\theta$ is given by
\begin{equation}
    \nabla_{\theta} L_{PINN}(\theta) = \frac{2}{N} \sum_{i=1}^N 
 (\nabla^2 u_{NN}(x_i; \theta) + f(x_i)) \cdot \nabla_{\theta} (\nabla^2 u_{NN}(x_i; \theta) + f(x_i))
\end{equation}
This involves third-order derivatives of $u_{NN}$ with respect to the spatial coordinates.
Despite requiring higher-order derivatives for the loss computation, PINNs often prove more effective in practice due to:
\begin{enumerate}
\item  \textbf{Direct Local Feedback:} The gradient directly pushes the network to reduce the local PDE residual. If the local residual is large, the gradient is large, indicating a clear direction to improve the fit. This provides stronger and more direct feedback to the optimizer, guiding it effectively towards satisfying the PDE at each sampled point. This local correction mechanism makes the optimization more stable and robust, even with stochastic sampling.
\item  \textbf{Consistent Gradient Signal:} While PINN loss also involves Monte Carlo sampling, the inherent objective at each sample point is to make the residual zero. The noise introduced by sampling is primarily in which points are selected for a mini-batch, rather than in the value of a global integral itself. This allows standard stochastic optimizers to work effectively, as they are designed to handle mini-batch stochasticity. The average gradient over a mini-batch still provides a reliable direction for reducing the PDE residual.
\end{enumerate}


\end{document}
