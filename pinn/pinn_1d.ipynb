{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3255e2a",
   "metadata": {},
   "source": [
    "### 1D PDE problem:\n",
    "\n",
    "$-u_{xx} + \\gamma u = f$\n",
    "\n",
    "and homogeneous boundary conditions (BC)\n",
    "\n",
    "#### Problem 1\n",
    "The analytical solution is\n",
    "\n",
    "$u(x) = \\sum_k c_k  \\sin(2 w_k  \\pi  x)$\n",
    "\n",
    "and\n",
    "\n",
    "$f = \\sum_k c_k  (4 w_k^2  \\pi^2 + \\gamma)  \\sin(2 w_k  \\pi  x)$\n",
    "\n",
    "#### Problem 2 (from [MscaleDNN](https://arxiv.org/abs/2007.11207))\n",
    "The analytical solution is\n",
    "\n",
    "$u(x) = e^{-x^2} \\sin(\\mu x)$\n",
    "\n",
    "and\n",
    "\n",
    "$f(x) = e^{-x^2} [(r + 4 \\mu^2 x^2 - 4 x^2 + 2) \\sin(\\mu x^2) + (8 \\mu x^2 - 2 Î¼) \\cos(\\mu x^2)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6e1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modules and device\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from utils import parse_args, get_activation, print_args, save_frame, make_video_from_frames\n",
    "from utils import is_notebook, cleanfiles, fourier_analysis, get_scheduler_generator, scheduler_step\n",
    "# from SOAP.soap import SOAP\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PDE\n",
    "class PDE:\n",
    "    def __init__(self, high=None, mu=70, r=0, problem=1):\n",
    "        # omega = [high]\n",
    "        omega = list(range(1, high + 1, 2))\n",
    "        # omega += [i + 50 for i in omega]\n",
    "        # omega = list(range(2, high + 1, 2))\n",
    "        # omega = [2**i for i in range(high.bit_length()) if 2**i <= high]\n",
    "        coeff = [1] * len(omega)\n",
    "\n",
    "        self.w = omega\n",
    "        self.c = coeff\n",
    "        self.mu = mu\n",
    "        self.r = r\n",
    "        if problem == 1:\n",
    "            self.f = self.f_1\n",
    "            self.u_ex = self.u_ex_1\n",
    "        else:\n",
    "            self.f = self.f_2\n",
    "            self.u_ex = self.u_ex_2\n",
    "\n",
    "    # Source term\n",
    "    def f_1(self, x):\n",
    "        y = torch.zeros_like(x)\n",
    "        for w, c in zip(self.w, self.c):\n",
    "            pi_w = 2 * torch.pi * w\n",
    "            y += c * (pi_w ** 2 + self.r) * torch.sin(pi_w * x)\n",
    "        return y\n",
    "\n",
    "    def f_2(self, x):\n",
    "        z = x ** 2\n",
    "        a = self.r + 4 * z * (self.mu ** 2 - 1) + 2\n",
    "        b = self.mu * z\n",
    "        c = 8 * b - 2 * self.mu\n",
    "        return torch.exp(-z) * (a * torch.sin(b) + c * torch.cos(b))\n",
    "\n",
    "    # Analytical solution\n",
    "    def u_ex_1(self, x):\n",
    "        y = torch.zeros_like(x)\n",
    "        for w, c in zip(self.w, self.c):\n",
    "            y += c * torch.sin(2 * w * torch.pi * x)\n",
    "        return y\n",
    "\n",
    "    def u_ex_2(self, x):\n",
    "        return torch.exp(-x**2) * torch.sin(self.mu * x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172c8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mesh\n",
    "class Mesh:\n",
    "    def __init__(self, ntrain, neval, ax, bx):\n",
    "        self.ntrain = ntrain\n",
    "        self.neval = neval\n",
    "        self.ax = ax\n",
    "        self.bx = bx\n",
    "        # training sample points (excluding the two points on the boundaries)\n",
    "        self.x_train = torch.linspace(self.ax, self.bx, self.ntrain + 1, device=device)[:-1].unsqueeze(-1)\n",
    "        self.x_eval = torch.linspace(self.ax, self.bx, self.neval + 1, device=device)[:-1].unsqueeze(-1)\n",
    "        self.pde = None\n",
    "        self.f = None\n",
    "        self.u_ex = None\n",
    "\n",
    "    def set_pde(self, pde: PDE):\n",
    "        self.pde = pde\n",
    "        # source term\n",
    "        self.f = pde.f(self.x_train)\n",
    "        # analytical solution\n",
    "        self.u_ex = pde.u_ex(self.x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a511f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one level NN\n",
    "class Level(nn.Module):\n",
    "    def __init__(self, dim_inputs, dim_outputs, dim_hidden: list,\n",
    "                 act: nn.Module = nn.Tanh()) -> None:\n",
    "        \"\"\"Simple neural network with linear layers and non-linear activation function\n",
    "        This class is used as universal function approximate for the solution of\n",
    "        partial differential equations using PINNs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_inputs = dim_inputs\n",
    "        self.dim_outputs = dim_outputs\n",
    "        # multi-layer MLP\n",
    "        layer_dim = [dim_inputs] + dim_hidden + [dim_outputs]\n",
    "        self.linear = nn.ModuleList([nn.Linear(layer_dim[i], layer_dim[i + 1])\n",
    "                                     for i in range(len(layer_dim) - 1)])\n",
    "        # activation function\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for i, layer in enumerate(self.linear):\n",
    "            x = layer(x)\n",
    "            # not applying nonlinear activation in the last layer\n",
    "            if i < len(self.linear) - 1:\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb08710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define level status\n",
    "class LevelStatus(Enum):\n",
    "    OFF = \"off\"\n",
    "    TRAIN = \"train\"\n",
    "    FROZEN = \"frozen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672bb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multilevel NN\n",
    "class MultiLevelNN(nn.Module):\n",
    "    def __init__(self, mesh: Mesh, num_levels: int, dim_inputs, dim_outputs, dim_hidden: list,\n",
    "                 act: nn.Module = nn.ReLU(), enforce_bc: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.mesh = mesh\n",
    "        # currently the same model on each level\n",
    "        self.models = nn.ModuleList([\n",
    "            Level(dim_inputs=dim_inputs, dim_outputs=dim_outputs, dim_hidden=dim_hidden, act=act)\n",
    "            for _ in range(num_levels)\n",
    "            ])\n",
    "        self.dim_inputs = dim_inputs\n",
    "        self.dim_outputs = dim_outputs\n",
    "        self.enforce_bc = enforce_bc\n",
    "\n",
    "        # All levels start as \"off\"\n",
    "        self.level_status = [LevelStatus.OFF] * num_levels\n",
    "\n",
    "        # No gradients are tracked initially\n",
    "        for model in self.models:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Scale factor\n",
    "        self.scales = [1.0] * num_levels\n",
    "\n",
    "    def get_status(self, level_idx: int):\n",
    "        if level_idx < 0 or level_idx >= self.num_levels():\n",
    "            raise IndexError(f\"Level index {level_idx} is out of range\")\n",
    "        return self.level_status[level_idx]\n",
    "\n",
    "    def set_status(self, level_idx: int, status: LevelStatus):\n",
    "        assert isinstance(status, LevelStatus), f\"Invalid status: {status}\"\n",
    "        if level_idx < 0 or level_idx >= self.num_levels():\n",
    "            raise IndexError(f\"Level index {level_idx} is out of range\")\n",
    "        self.level_status[level_idx] = status\n",
    "        requires_grad = status == LevelStatus.TRAIN\n",
    "        for param in self.models[level_idx].parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def set_all_status(self, status_list: list[LevelStatus]):\n",
    "        assert len(status_list) == len(self.models), \"Length mismatch in status list\"\n",
    "        for i, status in enumerate(status_list):\n",
    "            self.set_status(i, status)\n",
    "\n",
    "    def print_status(self):\n",
    "        for i, status in enumerate(self.level_status):\n",
    "            print(f\"Level {i}: {status.name}\")\n",
    "\n",
    "    def num_levels(self):\n",
    "        return len(self.models)\n",
    "\n",
    "    def num_active_levels(self) -> int:\n",
    "        \"\"\"Returns the number of levels currently active (train or frozen)\"\"\"\n",
    "        return sum(status != LevelStatus.OFF for status in self.level_status)\n",
    "\n",
    "    def set_scale(self, level_idx: int, scale: float):\n",
    "        if level_idx < 0 or level_idx >= self.num_levels():\n",
    "            raise IndexError(f\"Level index {level_idx} is out of range\")\n",
    "        self.scales[level_idx] = scale\n",
    "\n",
    "    def set_all_scales(self, scale_list: list[float]):\n",
    "        assert len(scale_list) == len(self.models), \"Length mismatch in scales\"\n",
    "        for i, scale in enumerate(scale_list):\n",
    "            self.set_scale(i, scale)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ys = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            if self.level_status[i] != LevelStatus.OFF:\n",
    "                x_scale = self.scales[i] * x\n",
    "                y = model.forward(x=x_scale)\n",
    "                ys.append(y)\n",
    "        if not ys:\n",
    "            # No active levels, return zeros with correct shape\n",
    "            return torch.zeros((x.shape[0], self.dim_outputs), device=x.device)\n",
    "        # Concatenate along the column (feature) dimension\n",
    "        out = torch.cat(ys, dim=1)\n",
    "        assert out.shape[1] == self.num_active_levels() * self.dim_outputs\n",
    "        return out\n",
    "\n",
    "    def get_solution(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.forward(x)\n",
    "        n_active = self.num_active_levels()\n",
    "        # reshape to [batch_size, num_levels, dim_outputs]\n",
    "        # and sum over levels\n",
    "        if n_active > 1:\n",
    "            y = y.view(-1, n_active, self.dim_outputs)\n",
    "            y = y.sum(dim=1)  # shape: (n, dim_outputs)\n",
    "        #\n",
    "        if self.enforce_bc:\n",
    "            g0 = self.mesh.u_ex[0].item()\n",
    "            g1 = self.mesh.u_ex[-1].item()\n",
    "            # in domain x in [0, 1]\n",
    "            y = g0 * (1 - x) + g1 * x + x * (1 - x) * y\n",
    "            # y = g0 + (x-0)/(1-0)*(g1 - g0) + (1 - torch.exp(0-x)) * (1 - torch.exp(x-1)) * y\n",
    "        return y\n",
    "\n",
    "    # def _init_weights(self, m):\n",
    "    #    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "    #        nn.init.ones_(m.weight)\n",
    "    #        m.bias.data.fill_(0.01)\n",
    "    #    if type(m) == nn.Linear:\n",
    "    #        torch.nn.init.xavier_uniform(m.weight)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7fa729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "class Loss:\n",
    "    def __init__(self, loss_type, loss_func=nn.MSELoss(), bc_weight=1.0):\n",
    "        self.loss_func = loss_func\n",
    "        self.type = loss_type\n",
    "        if self.type == -1:\n",
    "            self.name = \"Super Loss\"\n",
    "        elif self.type == 0:\n",
    "            self.name = \"PINN Loss\"\n",
    "        elif self.type == 1:\n",
    "            self.name = \"DRM Loss\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {self.type}\")\n",
    "        self.bc_weight = bc_weight\n",
    "\n",
    "    # \"Supervised\" loss against the analytical solution\n",
    "    def super_loss(self, model, mesh, loss_func):\n",
    "        x = mesh.x_train\n",
    "        u = model.get_solution(x)\n",
    "        loss = loss_func(u, mesh.u_ex)\n",
    "        return loss\n",
    "\n",
    "    # \"PINN\" loss\n",
    "    def pinn_loss(self, model, mesh, loss_func):\n",
    "        x = mesh.x_train.requires_grad_(True)\n",
    "        u = model.get_solution(x)\n",
    "\n",
    "        du_dx, = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)\n",
    "        d2u_dx2, = torch.autograd.grad(du_dx, x, grad_outputs=torch.ones_like(du_dx), create_graph=True)\n",
    "\n",
    "        # Internal loss\n",
    "        pde = mesh.pde\n",
    "        loss = loss_func(d2u_dx2[1:-1] + mesh.f[1:-1], pde.r * u[1:-1])\n",
    "        # Boundary loss\n",
    "        if not model.enforce_bc:\n",
    "            u_bc = u[[0, -1]]\n",
    "            u_ex_bc = mesh.u_ex[[0, -1]]\n",
    "            loss_b = loss_func(u_bc, u_ex_bc)\n",
    "            loss += self.bc_weight * loss_b\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def drm_loss(self, model, mesh: Mesh):\n",
    "        \"\"\"Deep Ritz Method loss\"\"\"\n",
    "        xs = mesh.x_train.requires_grad_(True)\n",
    "        u = model(xs)\n",
    "\n",
    "        grad_u_pred = torch.autograd.grad(u, xs,\n",
    "                                          grad_outputs=torch.ones_like(u),\n",
    "                                          create_graph=True)[0]\n",
    "\n",
    "        u_pred_sq = torch.sum(u**2, dim=1, keepdim=True)\n",
    "        grad_u_pred_sq = torch.sum(grad_u_pred**2, dim=1, keepdim=True)\n",
    "\n",
    "        f_val = mesh.pde.f(xs)\n",
    "        fu_prod = f_val * u\n",
    "\n",
    "        integrand_values = 0.5 * grad_u_pred_sq[1:-1] + 0.5 * mesh.pde.r * u_pred_sq[1:-1] - fu_prod[1:-1]\n",
    "        loss = torch.mean(integrand_values)\n",
    "\n",
    "        # Boundary loss\n",
    "        u_bc = u[[0, -1]]\n",
    "        u_ex_bc = mesh.u_ex[[0, -1]]\n",
    "        loss_b = self.loss_func(u_bc, u_ex_bc)\n",
    "        loss += self.bc_weight * loss_b\n",
    "\n",
    "        xs.requires_grad_(False)  # Disable gradient tracking for x\n",
    "        return loss\n",
    "\n",
    "    def loss(self, model, mesh):\n",
    "        if self.type == -1:\n",
    "            loss_value = self.super_loss(model=model, mesh=mesh, loss_func=self.loss_func)\n",
    "        elif self.type == 0:\n",
    "            loss_value = self.pinn_loss(model=model, mesh=mesh, loss_func=self.loss_func)\n",
    "        elif self.type == 1:\n",
    "            loss_value = self.drm_loss(model=model, mesh=mesh)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {self.type}\")\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de20f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train(model, mesh, criterion, iterations, adam_iterations, learning_rate, num_check, num_plots, sweep_idx,\n",
    "          level_idx, frame_dir, scheduler_gen):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = SOAP(model.parameters(), lr = 3e-3, betas=(.95, .95), weight_decay=.01,\n",
    "    #                  precondition_frequency=10)\n",
    "    scheduler = scheduler_gen(optimizer)\n",
    "    use_lbfgs = False\n",
    "\n",
    "    def to_np(t): return t.detach().cpu().numpy()\n",
    "\n",
    "    u_analytic = mesh.pde.u_ex(mesh.x_eval)\n",
    "    _, uf_analytic, _, _ = fourier_analysis(to_np(mesh.x_eval), to_np(u_analytic))\n",
    "    check_freq = (iterations + num_check - 1) // num_check\n",
    "    plot_freq = (iterations + num_plots - 1) // num_plots if num_plots > 0 else 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if i == adam_iterations:\n",
    "            use_lbfgs = True\n",
    "            optimizer = optim.LBFGS(model.parameters(), lr=learning_rate,\n",
    "                                    max_iter=20, tolerance_grad=1e-7, history_size=100)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion.loss(model=model, mesh=mesh)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        if use_lbfgs:\n",
    "            loss = optimizer.step(closure)\n",
    "        else:\n",
    "            # we need to set to zero the gradients of all model parameters (PyTorch accumulates grad by default)\n",
    "            optimizer.zero_grad()\n",
    "            # compute the loss value for the current batch of data\n",
    "            loss = criterion.loss(model=model, mesh=mesh)\n",
    "            # backpropagation to compute gradients of model param respect to the loss. computes dloss/dx\n",
    "            # for every parameter x which has requires_grad=True.\n",
    "            loss.backward()\n",
    "            # update the model param doing an optim step using the computed gradients and learning rate\n",
    "            optimizer.step()\n",
    "            #\n",
    "            scheduler_step(scheduler, loss)\n",
    "\n",
    "        if np.remainder(i + 1, check_freq) == 0 or i == iterations - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                u_eval = model.get_solution(mesh.x_eval)[:, 0].unsqueeze(-1)\n",
    "                error = u_analytic - u_eval.to(u_analytic.device)\n",
    "                print(f\"Iteration {i:6d}/{iterations:6d}, {criterion.name}: {loss.item():.4e}, \"\n",
    "                      f\"Err 2-norm: {torch.norm(error): .4e}, \"\n",
    "                      f\"inf-norm: {torch.max(torch.abs(error)):.4e}\")\n",
    "            model.train()\n",
    "\n",
    "        if plot_freq > 0 and (np.remainder(i + 1, plot_freq) == 0 or i == iterations - 1):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                u_train = model.get_solution(mesh.x_train)[:, 0].unsqueeze(-1)\n",
    "                u_eval = model.get_solution(mesh.x_eval)[:, 0].unsqueeze(-1)\n",
    "                error = u_analytic - u_eval.to(u_analytic.device)\n",
    "                xf_eval, uf_eval, _, _ = fourier_analysis(to_np(mesh.x_eval), to_np(u_eval))\n",
    "                save_frame(x=xf_eval, t=uf_analytic, y=uf_eval, xs=None,  ys=None,\n",
    "                           iteration=[sweep_idx, level_idx, i], title=\"Model_Frequencies\", frame_dir=frame_dir)\n",
    "                save_frame(x=to_np(mesh.x_eval), t=to_np(u_analytic), y=to_np(u_eval),\n",
    "                           xs=to_np(mesh.x_train), ys=to_np(u_train),\n",
    "                           iteration=[sweep_idx, level_idx, i], title=\"Model_Outputs\", frame_dir=frame_dir)\n",
    "                save_frame(x=to_np(mesh.x_eval), t=None, y=to_np(error), xs=None, ys=None,\n",
    "                           iteration=[sweep_idx, level_idx, i], title=\"Model_Errors\", frame_dir=frame_dir)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81628900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function\n",
    "def main(args=None):\n",
    "    # For reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    # Parse args\n",
    "    args = parse_args(args=args)\n",
    "    print_args(args=args)\n",
    "    # PDE\n",
    "    pde = PDE(high=args.high_freq, mu=args.mu, r=args.gamma,\n",
    "              problem=args.problem_id)\n",
    "    # Loss function [supervised with analytical solution (-1) or PINN loss (0)]\n",
    "    loss = Loss(loss_type=args.loss_type, bc_weight=args.bc_weight)\n",
    "    print(f\"Using loss: {loss.name}\")\n",
    "    # scheduler gen takes optimizer to return scheduler\n",
    "    scheduler_gen = get_scheduler_generator(args)\n",
    "    # 1-D mesh\n",
    "    mesh = Mesh(ntrain=args.nx, neval=args.nx_eval, ax=args.ax, bx=args.bx)\n",
    "    mesh.set_pde(pde=pde)\n",
    "    # Create an instance of multilevel model\n",
    "    # Input and output dimension: x -> u(x)\n",
    "    dim_inputs = 1\n",
    "    dim_outputs = 1\n",
    "    model = MultiLevelNN(mesh=mesh,\n",
    "                         num_levels=args.levels,\n",
    "                         dim_inputs=dim_inputs, dim_outputs=dim_outputs,\n",
    "                         dim_hidden=args.hidden_dims,\n",
    "                         act=get_activation(args.activation),\n",
    "                         enforce_bc=args.enforce_bc)\n",
    "    print(model)\n",
    "    model.to(device)\n",
    "    # Plotting\n",
    "    frame_dir = \"./frames\"\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    if args.clear:\n",
    "        cleanfiles(frame_dir)\n",
    "    num_plots = args.num_plots if args.plot else 0\n",
    "    # Train the model\n",
    "    for i in range(args.sweeps):\n",
    "        print(\"\\nTraining Sweep\", i)\n",
    "        # train each level at a time\n",
    "        for lev in range(args.levels):\n",
    "            # Turn all levels to \"frozen\" if they are not off\n",
    "            for k in range(args.levels):\n",
    "                if model.get_status(level_idx=k) != LevelStatus.OFF:\n",
    "                    model.set_status(level_idx=k, status=LevelStatus.FROZEN)\n",
    "            # Turn level l to \"train\"\n",
    "            model.set_status(level_idx=lev, status=LevelStatus.TRAIN)\n",
    "            print(\"\\nTraining Level\", lev)\n",
    "            model.print_status()\n",
    "            # set scale\n",
    "            scale = lev + 1\n",
    "            model.set_scale(level_idx=lev, scale=scale)\n",
    "            # Crank that !@#$ up\n",
    "            train(model=model, mesh=mesh, criterion=loss, iterations=args.epochs,\n",
    "                  adam_iterations=args.adam_epochs,\n",
    "                  learning_rate=args.lr, num_check=args.num_checks, num_plots=num_plots,\n",
    "                  sweep_idx=i, level_idx=lev, frame_dir=frame_dir, scheduler_gen=scheduler_gen)\n",
    "    # Turn PNGs into a video using OpenCV\n",
    "    if args.plot:\n",
    "        make_video_from_frames(frame_dir=frame_dir, name_prefix=\"Model_Outputs\",\n",
    "                               output_file=\"Solution.mp4\")\n",
    "        make_video_from_frames(frame_dir=frame_dir, name_prefix=\"Model_Errors\",\n",
    "                               output_file=\"Errors.mp4\")\n",
    "        make_video_from_frames(frame_dir=frame_dir, name_prefix=\"Model_Frequencies\",\n",
    "                               output_file=\"Frequencies.mp4\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70fa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options used:\n",
      "   --nx: 128\n",
      "   --nx_eval: 256\n",
      "   --num_checks: 20\n",
      "   --num_plots: 10\n",
      "   --epochs: 500\n",
      "   --sweeps: 1\n",
      "   --hidden_dims: [64, 64]\n",
      "   --ax: 0.0\n",
      "   --bx: 1.0\n",
      "   --high_freq: 16\n",
      "   --gamma: 0\n",
      "   --mu: 70\n",
      "   --lr: 0.001\n",
      "   --levels: 4\n",
      "   --loss_type: 0\n",
      "   --activation: tanh\n",
      "   --plot: True\n",
      "   --problem_id: 1\n",
      "Using loss: PINN Loss\n",
      "MultiLevelNN(\n",
      "  (models): ModuleList(\n",
      "    (0-3): 4 x Level(\n",
      "      (linear): ModuleList(\n",
      "        (0): Linear(in_features=1, out_features=64, bias=True)\n",
      "        (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "      )\n",
      "      (act): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Training Sweep 0\n",
      "\n",
      "Training Level 0\n",
      "Level 0: TRAIN\n",
      "Level 1: OFF\n",
      "Level 2: OFF\n",
      "Level 3: OFF\n",
      "Iteration     24/   500, PINN Loss: 3.2169e+06, Err 2-norm:  1.4183e+01, inf-norm: 1.9099e+00\n",
      "Iteration     49/   500, PINN Loss: 3.2155e+06, Err 2-norm:  3.3898e+01, inf-norm: 3.8774e+00\n",
      "Iteration     74/   500, PINN Loss: 3.2131e+06, Err 2-norm:  6.7689e+01, inf-norm: 7.2395e+00\n",
      "Iteration     99/   500, PINN Loss: 3.2074e+06, Err 2-norm:  8.5514e+01, inf-norm: 8.9946e+00\n",
      "Iteration    124/   500, PINN Loss: 3.1946e+06, Err 2-norm:  1.0433e+02, inf-norm: 1.1093e+01\n",
      "Iteration    149/   500, PINN Loss: 3.1572e+06, Err 2-norm:  1.3100e+02, inf-norm: 1.3091e+01\n",
      "Iteration    174/   500, PINN Loss: 3.0520e+06, Err 2-norm:  1.5461e+02, inf-norm: 1.4636e+01\n",
      "Iteration    199/   500, PINN Loss: 2.8880e+06, Err 2-norm:  1.4923e+02, inf-norm: 1.4447e+01\n",
      "Iteration    224/   500, PINN Loss: 2.6709e+06, Err 2-norm:  1.4130e+02, inf-norm: 1.4716e+01\n",
      "Iteration    249/   500, PINN Loss: 2.4928e+06, Err 2-norm:  1.4054e+02, inf-norm: 1.5255e+01\n",
      "Iteration    274/   500, PINN Loss: 2.4078e+06, Err 2-norm:  1.2592e+02, inf-norm: 1.4779e+01\n",
      "Iteration    299/   500, PINN Loss: 2.3528e+06, Err 2-norm:  1.1084e+02, inf-norm: 1.4214e+01\n",
      "Iteration    324/   500, PINN Loss: 2.2625e+06, Err 2-norm:  1.0362e+02, inf-norm: 1.4444e+01\n",
      "Iteration    349/   500, PINN Loss: 2.1296e+06, Err 2-norm:  9.2271e+01, inf-norm: 1.4342e+01\n",
      "Iteration    374/   500, PINN Loss: 1.9734e+06, Err 2-norm:  8.5571e+01, inf-norm: 1.4254e+01\n",
      "Iteration    399/   500, PINN Loss: 1.8409e+06, Err 2-norm:  8.0080e+01, inf-norm: 1.3675e+01\n",
      "Iteration    424/   500, PINN Loss: 1.7420e+06, Err 2-norm:  7.4690e+01, inf-norm: 1.2688e+01\n",
      "Iteration    449/   500, PINN Loss: 1.6652e+06, Err 2-norm:  6.9379e+01, inf-norm: 1.1836e+01\n",
      "Iteration    474/   500, PINN Loss: 1.6123e+06, Err 2-norm:  6.5194e+01, inf-norm: 1.1130e+01\n",
      "Iteration    499/   500, PINN Loss: 1.5741e+06, Err 2-norm:  6.2095e+01, inf-norm: 1.0610e+01\n",
      "\n",
      "Training Level 1\n",
      "Level 0: FROZEN\n",
      "Level 1: TRAIN\n",
      "Level 2: OFF\n",
      "Level 3: OFF\n",
      "Iteration     24/   500, PINN Loss: 1.5718e+06, Err 2-norm:  6.8114e+01, inf-norm: 1.2165e+01\n",
      "Iteration     49/   500, PINN Loss: 1.5681e+06, Err 2-norm:  8.3645e+01, inf-norm: 1.3791e+01\n",
      "Iteration     74/   500, PINN Loss: 1.5613e+06, Err 2-norm:  1.0647e+02, inf-norm: 1.5763e+01\n",
      "Iteration     99/   500, PINN Loss: 1.5534e+06, Err 2-norm:  1.3252e+02, inf-norm: 1.8059e+01\n",
      "Iteration    124/   500, PINN Loss: 1.5389e+06, Err 2-norm:  1.5719e+02, inf-norm: 2.0267e+01\n",
      "Iteration    149/   500, PINN Loss: 1.5030e+06, Err 2-norm:  1.7997e+02, inf-norm: 2.2014e+01\n",
      "Iteration    174/   500, PINN Loss: 1.3683e+06, Err 2-norm:  2.0707e+02, inf-norm: 2.3997e+01\n",
      "Iteration    199/   500, PINN Loss: 1.0364e+06, Err 2-norm:  2.4086e+02, inf-norm: 2.6777e+01\n",
      "Iteration    224/   500, PINN Loss: 7.2281e+05, Err 2-norm:  2.7338e+02, inf-norm: 2.9567e+01\n",
      "Iteration    249/   500, PINN Loss: 5.6351e+05, Err 2-norm:  2.8860e+02, inf-norm: 3.1167e+01\n",
      "Iteration    274/   500, PINN Loss: 3.9271e+05, Err 2-norm:  2.9513e+02, inf-norm: 3.2000e+01\n",
      "Iteration    299/   500, PINN Loss: 2.9460e+05, Err 2-norm:  2.9958e+02, inf-norm: 3.2623e+01\n",
      "Iteration    324/   500, PINN Loss: 2.5830e+05, Err 2-norm:  3.0143e+02, inf-norm: 3.3069e+01\n",
      "Iteration    349/   500, PINN Loss: 2.3370e+05, Err 2-norm:  3.0179e+02, inf-norm: 3.3128e+01\n",
      "Iteration    374/   500, PINN Loss: 2.1508e+05, Err 2-norm:  3.0128e+02, inf-norm: 3.3138e+01\n",
      "Iteration    399/   500, PINN Loss: 2.0100e+05, Err 2-norm:  3.0073e+02, inf-norm: 3.3146e+01\n",
      "Iteration    424/   500, PINN Loss: 1.8971e+05, Err 2-norm:  2.9983e+02, inf-norm: 3.3126e+01\n",
      "Iteration    449/   500, PINN Loss: 1.8039e+05, Err 2-norm:  2.9871e+02, inf-norm: 3.3069e+01\n",
      "Iteration    474/   500, PINN Loss: 1.7234e+05, Err 2-norm:  2.9762e+02, inf-norm: 3.3033e+01\n",
      "Iteration    499/   500, PINN Loss: 1.6490e+05, Err 2-norm:  2.9671e+02, inf-norm: 3.3047e+01\n",
      "\n",
      "Training Level 2\n",
      "Level 0: FROZEN\n",
      "Level 1: FROZEN\n",
      "Level 2: TRAIN\n",
      "Level 3: OFF\n",
      "Iteration     24/   500, PINN Loss: 1.6359e+05, Err 2-norm:  2.7700e+02, inf-norm: 3.0925e+01\n",
      "Iteration     49/   500, PINN Loss: 1.6035e+05, Err 2-norm:  2.4726e+02, inf-norm: 2.8514e+01\n",
      "Iteration     74/   500, PINN Loss: 1.5615e+05, Err 2-norm:  2.1725e+02, inf-norm: 2.6378e+01\n",
      "Iteration     99/   500, PINN Loss: 1.5404e+05, Err 2-norm:  2.0835e+02, inf-norm: 2.5475e+01\n",
      "Iteration    124/   500, PINN Loss: 1.5142e+05, Err 2-norm:  2.0277e+02, inf-norm: 2.4699e+01\n",
      "Iteration    149/   500, PINN Loss: 1.4637e+05, Err 2-norm:  1.9932e+02, inf-norm: 2.4172e+01\n",
      "Iteration    174/   500, PINN Loss: 1.3185e+05, Err 2-norm:  1.9340e+02, inf-norm: 2.3263e+01\n",
      "Iteration    199/   500, PINN Loss: 1.0660e+05, Err 2-norm:  1.9882e+02, inf-norm: 2.2998e+01\n",
      "Iteration    224/   500, PINN Loss: 8.1441e+04, Err 2-norm:  2.1190e+02, inf-norm: 2.3612e+01\n",
      "Iteration    249/   500, PINN Loss: 6.8237e+04, Err 2-norm:  2.3260e+02, inf-norm: 2.5623e+01\n",
      "Iteration    274/   500, PINN Loss: 6.0527e+04, Err 2-norm:  2.4867e+02, inf-norm: 2.8330e+01\n",
      "Iteration    299/   500, PINN Loss: 5.2007e+04, Err 2-norm:  2.5787e+02, inf-norm: 2.9752e+01\n",
      "Iteration    324/   500, PINN Loss: 4.1178e+04, Err 2-norm:  2.6352e+02, inf-norm: 3.0514e+01\n",
      "Iteration    349/   500, PINN Loss: 3.1100e+04, Err 2-norm:  2.6773e+02, inf-norm: 3.1204e+01\n",
      "Iteration    374/   500, PINN Loss: 2.4822e+04, Err 2-norm:  2.6849e+02, inf-norm: 3.1389e+01\n",
      "Iteration    399/   500, PINN Loss: 2.0348e+04, Err 2-norm:  2.6903e+02, inf-norm: 3.1525e+01\n",
      "Iteration    424/   500, PINN Loss: 1.6890e+04, Err 2-norm:  2.6884e+02, inf-norm: 3.1581e+01\n",
      "Iteration    449/   500, PINN Loss: 1.4836e+04, Err 2-norm:  2.6870e+02, inf-norm: 3.1577e+01\n",
      "Iteration    474/   500, PINN Loss: 1.3914e+04, Err 2-norm:  2.6906e+02, inf-norm: 3.1593e+01\n",
      "Iteration    499/   500, PINN Loss: 1.3403e+04, Err 2-norm:  2.6994e+02, inf-norm: 3.1652e+01\n",
      "\n",
      "Training Level 3\n",
      "Level 0: FROZEN\n",
      "Level 1: FROZEN\n",
      "Level 2: FROZEN\n",
      "Level 3: TRAIN\n",
      "Iteration     24/   500, PINN Loss: 1.3312e+04, Err 2-norm:  2.6176e+02, inf-norm: 3.0315e+01\n",
      "Iteration     49/   500, PINN Loss: 1.3176e+04, Err 2-norm:  2.4565e+02, inf-norm: 2.8560e+01\n",
      "Iteration     74/   500, PINN Loss: 1.2939e+04, Err 2-norm:  2.1864e+02, inf-norm: 2.5515e+01\n",
      "Iteration     99/   500, PINN Loss: 1.2765e+04, Err 2-norm:  1.9965e+02, inf-norm: 2.3708e+01\n",
      "Iteration    124/   500, PINN Loss: 1.2579e+04, Err 2-norm:  1.9062e+02, inf-norm: 2.2308e+01\n",
      "Iteration    149/   500, PINN Loss: 1.2176e+04, Err 2-norm:  1.8122e+02, inf-norm: 2.0928e+01\n",
      "Iteration    174/   500, PINN Loss: 1.1106e+04, Err 2-norm:  1.6892e+02, inf-norm: 1.9089e+01\n",
      "Iteration    199/   500, PINN Loss: 9.5415e+03, Err 2-norm:  1.4728e+02, inf-norm: 1.6177e+01\n",
      "Iteration    224/   500, PINN Loss: 8.3758e+03, Err 2-norm:  1.2469e+02, inf-norm: 1.3660e+01\n",
      "Iteration    249/   500, PINN Loss: 7.1909e+03, Err 2-norm:  1.0806e+02, inf-norm: 1.1873e+01\n",
      "Iteration    274/   500, PINN Loss: 6.9245e+03, Err 2-norm:  9.6894e+01, inf-norm: 1.1054e+01\n",
      "Iteration    299/   500, PINN Loss: 6.7829e+03, Err 2-norm:  8.9618e+01, inf-norm: 1.0301e+01\n",
      "Iteration    324/   500, PINN Loss: 6.5663e+03, Err 2-norm:  8.1135e+01, inf-norm: 9.3726e+00\n",
      "Iteration    349/   500, PINN Loss: 6.2988e+03, Err 2-norm:  7.3451e+01, inf-norm: 8.5357e+00\n",
      "Iteration    374/   500, PINN Loss: 5.9034e+03, Err 2-norm:  6.5512e+01, inf-norm: 7.5767e+00\n",
      "Iteration    399/   500, PINN Loss: 5.5258e+03, Err 2-norm:  5.7247e+01, inf-norm: 6.5984e+00\n",
      "Iteration    424/   500, PINN Loss: 5.1437e+03, Err 2-norm:  4.9971e+01, inf-norm: 5.6003e+00\n",
      "Iteration    449/   500, PINN Loss: 4.8069e+03, Err 2-norm:  4.5113e+01, inf-norm: 5.0387e+00\n",
      "Iteration    474/   500, PINN Loss: 4.5118e+03, Err 2-norm:  4.1861e+01, inf-norm: 4.8962e+00\n",
      "Iteration    499/   500, PINN Loss: 4.2625e+03, Err 2-norm:  3.8321e+01, inf-norm: 4.5656e+00\n",
      "  Video saved as ./frames/Solution.mp4\n",
      "  Video saved as ./frames/Errors.mp4\n"
     ]
    }
   ],
   "source": [
    "# can run it like normal: python filename.py\n",
    "if __name__ == \"__main__\":\n",
    "    if is_notebook():\n",
    "        err = main(['--levels', '4', '--epochs', '10000', '--sweeps', '1', '--plot'])\n",
    "    else:\n",
    "        err = main()\n",
    "    try:\n",
    "        import sys\n",
    "        sys.exit(err)\n",
    "    except SystemExit:\n",
    "        pass  # Prevent traceback in Jupyter or VS Code"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pymfem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
